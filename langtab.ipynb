{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "This cookbook was created by Joe Constantino on June 25, 2024. Shout out to Patrick Green for doing the initial work to prove how you can query Headless BI from a notebook.\n",
        "\n",
        "This cookbook can be configured to query a Tableau Published data source in natural language using the Headless BI REST endpoint. It uses langchain libraries and the langchain development framework to generate a headless BI request payload from a natural language input query. It then executes a chain to generate an answer to the user's input query.\n",
        "\n",
        "More on the motivation for this prototype can be found in:\n",
        "\n",
        "\n",
        "*   [Demo](https://drive.google.com/drive/u/0/folders/1drsQBnkA2ZQAgP2Okwo19t9Xdl-wuisM) and [explainer deck](https://docs.google.com/presentation/d/1_MoYuf28cKNtASp10aW3RJbBcIXSl8vnnijQXqzWWjI/edit#slide=id.g2e7ddaa96e5_0_220)\n",
        "*   PRD (still a work in progress)\n",
        "[link text](https://docs.google.com/document/d/1S3gWP4b_ZzF_a4uMcG-zZ1Uu7B3mph78VyvAza5hxyQ/edit#heading=h.dp8s1hcyd5za)\n",
        "\n",
        "\n",
        "To get started with this notebook, make a copy and add your keys to the vault. More info on headless BI configuration requirements can be found [here](https://docs.google.com/document/d/1_um4JzBCYYtzAECduSTKmeY-COQSvAZo5ISyobR4LPY/edit) and [here](https://docs.google.com/document/d/1C5uCVhelRBKO9L40Asm1Cr3EbXBkLPINbxEYdVEvhII/edit#heading=h.fsq4357uza2)."
      ],
      "metadata": {
        "id": "_Rj_FTCWyho2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfC3HzTxtcJl",
        "outputId": "49520f00-f023-4381-bcce-87958781196c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PsAfFpNPutLh",
        "outputId": "a55a92d7-ea98-43b6-976b-df72df28c21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.9-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3,>=0.2.2 (from langchain_openai)\n",
            "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.2->langchain_openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.2->langchain_openai)\n",
            "  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: orjson, jsonpointer, h11, tiktoken, jsonpatch, httpcore, langsmith, httpx, openai, langchain-core, langchain_openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.9 langchain_openai-0.1.9 langsmith-0.1.82 openai-1.35.3 orjson-3.10.5 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_experimental"
      ],
      "metadata": {
        "id": "Hk9l4Avmlikx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d05b7e8-e421-40cc-c994-36f383452d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.61-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community<0.3.0,>=0.2.5 (from langchain_experimental)\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.2.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.5 (from langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (8.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.7.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (3.0.0)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.5->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community, langchain_experimental\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.5 langchain-community-0.2.5 langchain-text-splitters-0.2.1 langchain_experimental-0.0.61 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import all required libraries\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "# you only need to mount your drive if you have stored the headless BI system prompt in your drive\n",
        "#drive.mount('/content/drive')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openai')\n"
      ],
      "metadata": {
        "id": "IJWw_bx_tpod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read in the datasource metadata of the datasource that is listed in vault\n",
        "def read():\n",
        "    url = userdata.get('readMetadata')\n",
        "    payload = json.dumps({\n",
        "        \"connection\": {\n",
        "            \"tableauServerName\": userdata.get('tableauServerName'),\n",
        "            \"siteId\": userdata.get('siteId'),\n",
        "            \"datasource\": userdata.get('datasource')\n",
        "        },\n",
        "    })\n",
        "\n",
        "    headers = {\n",
        "    'Credential-Key': userdata.get('patKey'),\n",
        "    'Credential-value': userdata.get('patToken'),\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Print the response data\n",
        "        #print(\"Response Data:\")\n",
        "        #print(response.json())\n",
        "        data = response.json()['data']\n",
        "        # Create a pandas DataFrame from the JSON data\n",
        "        #df = pd.DataFrame(data)\n",
        "\n",
        "        # Display the first few rows of the DataFrame\n",
        "        #print(\"Table view of data from the public REST API:\")\n",
        "        #print(df.head())\n",
        "        #print(df.all())\n",
        "        return data\n",
        "        #display(df.head())\n",
        "    else:\n",
        "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
        "        print(response.text)\n",
        "datasource_metadata = read()"
      ],
      "metadata": {
        "id": "KfyVOfaxzWrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the headless BI query template\n",
        "def send(query):\n",
        "    url = userdata.get('vdsUrl')\n",
        "    payload = json.dumps({\n",
        "        \"connection\": {\n",
        "            \"tableauServerName\": userdata.get('tableauServerName'),\n",
        "            \"siteId\": userdata.get('siteId'),\n",
        "            \"datasource\": userdata.get('datasource')\n",
        "        },\n",
        "        \"query\": query\n",
        "    })\n",
        "\n",
        "    headers = {\n",
        "    'Credential-Key': userdata.get('patKey'),\n",
        "    'Credential-value': userdata.get('patToken'),\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Print the response data\n",
        "        #print(\"Response Data:\")\n",
        "        #print(response.json())\n",
        "        data = response.json()['data']\n",
        "        # Create a pandas DataFrame from the JSON data\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Display the first few rows of the DataFrame\n",
        "        #print(\"Table view of data from the public REST API:\")\n",
        "        #print(df.head())\n",
        "        #print(df.all())\n",
        "        return df\n",
        "        #display(df.head())\n",
        "    else:\n",
        "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
        "        print(response.text)"
      ],
      "metadata": {
        "id": "8vihYtKpuy_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and instantiate prompt; link to base system prompt: https://drive.google.com/file/d/1wqmTSKIwDPgpucwLAgalEyumO2egDJJ5/view?usp=drive_link\n",
        "\n",
        "# Specify the path to the system prompt\n",
        "path = '/content/drive/My Drive/Colab Notebooks/headless_bi_prompt_schema.json'\n",
        "\n",
        "# read in system prompt\n",
        "with open(path, 'r') as json_file:\n",
        "    headless_bi_prompt = json.load(json_file)\n",
        "\n",
        "# add datasource metadata of the connected datasource to the system prompt\n",
        "headless_bi_prompt['data_model'] = datasource_metadata\n",
        "headless_bi_prompt_string = json.dumps(headless_bi_prompt)\n"
      ],
      "metadata": {
        "id": "YoQOU_fmvelA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_utterance():\n",
        "  query = input('what would you like to know about your data? reply stop if you are done.')\n",
        "  return query\n"
      ],
      "metadata": {
        "id": "3AivGN7iIe4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_utterance = get_utterance()\n",
        "while (active_utterance != 'stop'):\n",
        "  llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "  active_prompt_template = ChatPromptTemplate.from_messages([\n",
        "      SystemMessage(content=headless_bi_prompt_string),\n",
        "      (\"user\", \"{utterance}\")])\n",
        "  #get_utterance = input('what would you like to know about your data? ')\n",
        "  prompt_value = active_prompt_template.invoke(\n",
        "      {\n",
        "          \"utterance\": active_utterance\n",
        "      }\n",
        "  )\n",
        "  output_parser = StrOutputParser()\n",
        "  chain = active_prompt_template | llm | output_parser\n",
        "  output = chain.invoke(active_utterance)\n",
        "\n",
        "  #print reasoning\n",
        "  print(output.split('JSON_payload')[0])\n",
        "\n",
        "  #parse LLM output and query headless BI\n",
        "  parsed_output = output.split('JSON_payload')[1]\n",
        "  match = re.search(r'{.*}', parsed_output, re.DOTALL)\n",
        "  if match:\n",
        "    json_string = match.group(0)\n",
        "    payload = json.loads(json_string)\n",
        "  #print(payload)\n",
        "  df = send(payload)\n",
        "  agent = create_pandas_dataframe_agent(\n",
        "      ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\"),\n",
        "      df,\n",
        "      verbose=True,\n",
        "      agent_type=AgentType.OPENAI_FUNCTIONS,\n",
        "      allow_dangerous_code=True\n",
        "  )\n",
        "  instruct_header = \"answer the following query directly by executing the necessary python code. Give a succinct explanation of the steps you took and how you know the answer is correct: \"\n",
        "  agent.invoke(instruct_header + active_utterance)\n",
        "  active_utterance = get_utterance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv57JrlE6_zq",
        "outputId": "73a7549f-9657-4987-bdef-13b90e49a98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what would you like to know about your data? reply stop if you are done.which quarter from the first year of the data had the most technology profit?\n",
            "Reasoning: \n",
            "To determine which quarter from the first year of the data had the most technology profit, I need to include the following columns in the query:\n",
            "1. Quarter - This can be derived from the \"Order Date\" column using the \"DATE_QTR\" function.\n",
            "2. Profit - This can be aggregated using the \"SUM\" function.\n",
            "3. Category - To filter for the \"Technology\" category.\n",
            "\n",
            "Additionally, I need to apply two filters:\n",
            "1. Filter the data to include only the first year.\n",
            "2. Filter the data to include only the \"Technology\" category.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df[df['QUARTER(Order Date)'] == 1].loc[df[df['QUARTER(Order Date)'] == 1]['SUM(Profit)'].idxmax()]\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mQUARTER(Order Date)        1.0000\n",
            "SUM(Profit)            15872.2573\n",
            "Name: 2, dtype: float64\u001b[0m\u001b[32;1m\u001b[1;3mThe quarter from the first year of the data (quarter 1) that had the most technology profit is quarter 1 with a profit of $15,872.26. \n",
            "\n",
            "I filtered the dataframe to only include data from quarter 1, then located the row with the highest profit using the `idxmax()` function.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "what would you like to know about your data? reply stop if you are done.can you show me technology profit in each quarter of the first year of the data to confirm?\n",
            "Reasoning: \n",
            "To retrieve the profit for the \"Technology\" category in each quarter of the first year of the dataset, we need the following columns: \"Category\", \"Profit\", and \"Order Date\". We will filter the data to only include rows where \"Category\" is \"Technology\" and the \"Order Date\" falls within the first year. Then, we will aggregate the profit by quarter.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['QUARTER(Order Date)'] = pd.to_datetime(df['QUARTER(Order Date)'])\\nfirst_year_technology_profit = df[(df['Category'] == 'Technology') & (df['QUARTER(Order Date)'].dt.year == 2024)]['SUM(Profit)']\\nfirst_year_technology_profit\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mNameError: name 'pd' is not defined\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"import pandas as pd\\n\\ndf['QUARTER(Order Date)'] = pd.to_datetime(df['QUARTER(Order Date)'])\\nfirst_year_technology_profit = df[(df['Category'] == 'Technology') & (df['QUARTER(Order Date)'].dt.year == 2024)]['SUM(Profit)']\\nfirst_year_technology_profit\"}`\n",
            "responded: It seems that the pandas library has not been imported yet. Let me fix that and rerun the query.\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m0    15872.26\n",
            "1     4599.93\n",
            "2    19023.31\n",
            "3    11338.58\n",
            "Name: SUM(Profit), dtype: float64\u001b[0m\u001b[32;1m\u001b[1;3mThe technology profit in each quarter of the first year of the data is as follows:\n",
            "- Quarter 1: $15,872.26\n",
            "- Quarter 2: $4,599.93\n",
            "- Quarter 3: $19,023.31\n",
            "- Quarter 4: $11,338.58\n",
            "\n",
            "These values confirm the profits for the Technology category in each quarter of the first year of the data.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "what would you like to know about your data? reply stop if you are done.thanks. within the technology category, which product has the most profit? How does that compare to the product with the most sales?\n",
            "Reasoning: To answer this query, we need to retrieve the product with the highest profit and compare it to the product with the highest sales within the Technology category. This will involve the following steps:\n",
            "1. Retrieve products within the Technology category.\n",
            "2. Identify the product with the highest profit.\n",
            "3. Identify the product with the highest sales.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df[df['Product Name'].str.contains('Technology')].nlargest(1, 'SUM(Profit)')\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m                                          Product Name  SUM(Profit)  \\\n",
            "105  Logitech G430 Surround Sound Gaming Headset wi...     377.5528   \n",
            "\n",
            "     SUM(Sales)  \n",
            "105    1247.844  \u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df[df['Product Name'].str.contains('Technology')].nlargest(1, 'SUM(Sales)')\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m                                          Product Name  SUM(Profit)  \\\n",
            "105  Logitech G430 Surround Sound Gaming Headset wi...     377.5528   \n",
            "\n",
            "     SUM(Sales)  \n",
            "105    1247.844  \u001b[0m\u001b[32;1m\u001b[1;3mThe product within the technology category that has the most profit is the \"Logitech G430 Surround Sound Gaming Headset\" with a profit of 377.55. Interestingly, this is the same product that has the most sales within the technology category, with sales amounting to 1247.84.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "what would you like to know about your data? reply stop if you are done.stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "958jC30yhEBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}