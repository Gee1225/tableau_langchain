{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rj_FTCWyho2"
   },
   "source": [
    "# README\n",
    "This cookbook was created by Joe Constantino on June 25, 2024. Shout out to Patrick Green for doing the initial work to prove how you can query Headless BI from a notebook.\n",
    "\n",
    "This cookbook can be configured to query a Tableau Published data source in natural language using the Headless BI REST endpoint. It uses langchain libraries and the langchain development framework to generate a headless BI request payload from a natural language input query. It then executes a chain to generate an answer to the user's input query.\n",
    "\n",
    "More on the motivation for this prototype can be found in:\n",
    "\n",
    "\n",
    "*   [Demo](https://drive.google.com/drive/u/0/folders/1drsQBnkA2ZQAgP2Okwo19t9Xdl-wuisM) and [explainer deck](https://docs.google.com/presentation/d/1_MoYuf28cKNtASp10aW3RJbBcIXSl8vnnijQXqzWWjI/edit#slide=id.g2e7ddaa96e5_0_220)\n",
    "*   PRD (still a work in progress)\n",
    "[link text](https://docs.google.com/document/d/1S3gWP4b_ZzF_a4uMcG-zZ1Uu7B3mph78VyvAza5hxyQ/edit#heading=h.dp8s1hcyd5za)\n",
    "\n",
    "\n",
    "To get started with this notebook, make a copy and add your keys to the vault. More info on headless BI configuration requirements can be found [here](https://docs.google.com/document/d/1_um4JzBCYYtzAECduSTKmeY-COQSvAZo5ISyobR4LPY/edit) and [here](https://docs.google.com/document/d/1C5uCVhelRBKO9L40Asm1Cr3EbXBkLPINbxEYdVEvhII/edit#heading=h.fsq4357uza2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfC3HzTxtcJl",
    "outputId": "49520f00-f023-4381-bcce-87958781196c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PsAfFpNPutLh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a55a92d7-ea98-43b6-976b-df72df28c21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.9-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.3,>=0.2.2 (from langchain_openai)\n",
      "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n",
      "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.2->langchain_openai)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.2->langchain_openai)\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai)\n",
      "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
      "Installing collected packages: orjson, jsonpointer, h11, tiktoken, jsonpatch, httpcore, langsmith, httpx, openai, langchain-core, langchain_openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.9 langchain_openai-0.1.9 langsmith-0.1.82 openai-1.35.3 orjson-3.10.5 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "# %pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hk9l4Avmlikx",
    "outputId": "4d05b7e8-e421-40cc-c994-36f383452d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.0.61-py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community<0.3.0,>=0.2.5 (from langchain_experimental)\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.2.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.5 (from langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (8.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.7.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (3.0.0)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.5->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.0.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community, langchain_experimental\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-0.2.5 langchain-community-0.2.5 langchain-text-splitters-0.2.1 langchain_experimental-0.0.61 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# %pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IJWw_bx_tpod"
   },
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# loading environment variables\n",
    "load_dotenv()\n",
    "env_vars = os.environ\n",
    "\n",
    "# you only need to mount your drive if you have stored the headless BI system prompt in your drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KfyVOfaxzWrL"
   },
   "outputs": [],
   "source": [
    "#read in the datasource metadata of the datasource that is listed in vault\n",
    "def read():\n",
    "    url = env_vars['READ_METADATA']\n",
    "    payload = json.dumps({\n",
    "        \"connection\": {\n",
    "            \"tableauServerName\": env_vars['TABLEAU_DOMAIN'],\n",
    "            \"siteId\": env_vars['SITE_NAME'],\n",
    "            \"datasource\": env_vars['DATA_SOURCE']\n",
    "        },\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Credential-Key': env_vars['PAT_NAME'],\n",
    "    'Credential-value': env_vars['PAT_SECRET'],\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Print the response data\n",
    "        #print(\"Response Data:\")\n",
    "        #print(response.json())\n",
    "        data = response.json()['data']\n",
    "        # Create a pandas DataFrame from the JSON data\n",
    "        #df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the first few rows of the DataFrame\n",
    "        #print(\"Table view of data from the public REST API:\")\n",
    "        #print(df.head())\n",
    "        #print(df.all())\n",
    "        return data\n",
    "        #display(df.head())\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
    "        print(response.text)\n",
    "datasource_metadata = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8vihYtKpuy_5"
   },
   "outputs": [],
   "source": [
    "#define the headless BI query template\n",
    "def send(query):\n",
    "    url = env_vars['VDS_URL']\n",
    "    payload = json.dumps({\n",
    "        \"connection\": {\n",
    "            \"tableauServerName\": env_vars['TABLEAU_DOMAIN'],\n",
    "            \"siteId\": env_vars['SITE_NAME'],\n",
    "            \"datasource\": env_vars['DATA_SOURCE']\n",
    "        },\n",
    "        \"query\": query\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Credential-Key': env_vars['PAT_NAME'],\n",
    "    'Credential-value': env_vars['PAT_SECRET'],\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Print the response data\n",
    "        #print(\"Response Data:\")\n",
    "        #print(response.json())\n",
    "        data = response.json()['data']\n",
    "        # Create a pandas DataFrame from the JSON data\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the first few rows of the DataFrame\n",
    "        #print(\"Table view of data from the public REST API:\")\n",
    "        #print(df.head())\n",
    "        #print(df.all())\n",
    "        return df\n",
    "        #display(df.head())\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YoQOU_fmvelA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instructions\": \"You are an expert at writing JSON payloads for VDS queries. \\nVDS queries are passed in as a JSON body that describes the query, and the return is passed back as an array of JSON objects. The VDS query is a JSON object. It contains three fundamental components.\\n1. columns [required] - an array of columns that define the desired output of the query\\n2. filters [optional] - an array of filters to apply to the query. They can include fields that are not in the columns array.\\n3. options [optional] - metadata that can be used to adjust the behavior of the query. As of now, this is just whether the return format is OBJECTS or ARRAYS (objects is human readable, arrays is more compact).\\nYour task is to retrieve data relevant to a natural language query. A pandas AI Agent will be used to transform and analyze the data that your VDS query returns. Don't try to do too much with the json query. You will be successful if you bring back all the data that is required to answer the question, even if additional transformation and actions are needed. Be greedy. Query as much data as might be useful. You will construct a VDS query from a natural language query.\\nYou will ground the natural language query in the data model provided in the prompt. \\nUse the natural language query and the provided datamodel to generate the appropriate JSON object for a VDS query.\\nKeep your ouput very structured. Use the following structure:\\nReasoning: \\nJSON_payload: \\nexplanation:\\nMake sure you use this structure so that it's simple to parse the output.\\n\", \"user_query\": \"\", \"vds_schema\": {\"Column\": {\"type\": \"object\", \"anyOf\": [{\"required\": [\"columnName\"]}, {\"required\": [\"columnName\", \"function\"]}, {\"required\": [\"columnName\", \"calculation\"]}], \"properties\": {\"columnName\": {\"type\": \"string\", \"description\": \"The name of the column which must be supplied. Either a reference to a specific column in the data source, or in the case of a calculation a user supplied name for the calculation.\"}, \"columnAlias\": {\"type\": \"string\", \"description\": \"An alternate name to give the column. Will only be used in Object format output.\"}, \"maxDecimalPlaces\": {\"type\": \"integer\", \"description\": \"The maximum number of decimal places. Any trailing 0s will be dropped. The maxDecimalPlaces value must be greater or equal to 0.\"}, \"sortDirection\": {\"allOf\": [{\"$ref\": \"#/components/schemas/SortDirection\"}, {\"description\": \"The direction of the sort, either ascending or descending. If not supplied the default is ascending\"}]}, \"sortPriority\": {\"type\": \"integer\", \"description\": \"To enable sorting on a specific Column provide a sortPriority for that Column, and that Column will be sorted. The sortPriority provides a ranking of how to sort Columns when multiple Columns are being sorted. The highest priority (lowest number) Column is sorted first. If only 1 Column is being sorted, then any value may be used for sortPriority. SortPriority should be an integer greater than 0.\"}, \"function\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Function\"}, {\"description\": \"Provide a Function for a Column to generate an aggregation against that Columns' values. For example providing the SUM Function will cause an aggregated SUM to be calculated for that Column.\"}]}, \"calculation\": {\"type\": \"string\", \"description\": \"Provide a Calculation to generate a new data Column based on that Calculation. The Calcuation should contain a string based on the Tableau Calculated Field Syntax. Since this is a newly generated Column, you must give it its own unique Column Name. Also a Column cannot contain both a Function, and a Calculation.\"}}}, \"ColumnMetadata\": {\"type\": \"object\", \"description\": \"Describes a column in the datasource that can be used to create queries.\", \"properties\": {\"columnName\": {\"type\": \"string\"}, \"caption\": {\"type\": \"string\"}, \"dataType\": {\"type\": \"string\", \"enum\": [\"UNSPECIFIED\", \"INTEGER\", \"REAL\", \"STRING\", \"DATETIME\", \"BOOLEAN\", \"DATE\", \"SPATIAL\", \"UNKNOWN\", \"UNRECOGNIZED\"]}, \"columnContainsNulls\": {\"type\": \"boolean\"}, \"objectGraphId\": {\"type\": \"string\"}}}, \"DateObject\": {\"type\": \"object\", \"required\": [\"day\", \"month\", \"year\"], \"properties\": {\"day\": {\"type\": \"integer\"}, \"month\": {\"type\": \"integer\"}, \"year\": {\"type\": \"integer\"}}}, \"Filter\": {\"type\": \"object\", \"required\": [\"filterType\"], \"properties\": {\"columnName\": {\"type\": \"string\"}, \"column\": {\"allOf\": [{\"$ref\": \"#/components/schemas/FilterColumn\"}]}, \"filterType\": {\"type\": \"string\", \"enum\": [\"QUANTITATIVE\", \"SET\", \"DATE\", \"TOP\"]}, \"context\": {\"type\": \"boolean\", \"default\": \"false\"}}, \"discriminator\": {\"propertyName\": \"filterType\", \"mapping\": {\"QUANTITATIVE\": \"#/components/schemas/QuantitativeFilter\", \"SET\": \"#/components/schemas/SetFilter\", \"DATE\": \"#/components/schemas/RelativeDateFilter\", \"TOP\": \"#/components/schemas/TopNFilter\"}}}, \"FilterColumn\": {\"type\": \"object\", \"oneOf\": [{\"required\": [\"columnName\", \"function\"]}, {\"required\": [\"calculation\"]}], \"properties\": {\"columnName\": {\"type\": \"string\"}, \"function\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Function\"}]}, \"calculation\": {\"type\": \"string\"}}}, \"Function\": {\"type\": \"string\", \"description\": \"The standard set of Tableau aggregations.\", \"enum\": [\"SUM\", \"AVG\", \"MEDIAN\", \"COUNT\", \"COUNT_DIST\", \"MIN\", \"MAX\", \"STD_DEV\", \"VARIANCE\", \"CLCT\", \"DATE_YEAR\", \"DATE_QTR\", \"DATE_MONTH\", \"DATE_WEEK\", \"DATE_DAY\", \"DATE_TRUNC_YEAR\", \"DATE_TRUNC_QTR\", \"DATE_TRUNC_MONTH\", \"DATE_TRUNC_WEEK\", \"DATE_TRUNC_DAY\"]}, \"MetadataOutput\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/ColumnMetadata\"}}}}, \"QuantitativeFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"quantitativeFilterType\"], \"properties\": {\"quantitativeFilterType\": {\"type\": \"string\", \"enum\": [\"RANGE\", \"MIN\", \"MAX\", \"SPECIAL\"]}, \"min\": {\"type\": \"number\", \"description\": \"A numerical value, either integer or floating point indicating the minimum value to filter upon. Required for RANGE and MIN\"}, \"max\": {\"type\": \"number\", \"description\": \"A numerical value, either integer or floating point indicating the maximum value to filter upon. Required for RANGE and MAX\"}, \"minDate\": {\"$ref\": \"#/components/schemas/DateObject\"}, \"maxDate\": {\"$ref\": \"#/components/schemas/DateObject\"}, \"quantitativeFilterIncludedValues\": {\"allOf\": [{\"$ref\": \"#/components/schemas/QuantitativeFilterIncludedValues\"}]}}}]}, \"QuantitativeFilterIncludedValues\": {\"type\": \"string\", \"enum\": [\"ALL\", \"NON_NULL\", \"NULL\", \"IN_RANGE\", \"IN_RANGE_OR_NULL\", \"NONE\"]}, \"Query\": {\"description\": \"The Query is the fundamental interface to VDS. It holds the specific semantics to perform against the Data Source. A Query consists of an array of Columns to query against, an optional array of filters to apply to the query, and an optional Metadata field to modify the query behavior.\", \"required\": [\"columns\"], \"type\": \"object\", \"properties\": {\"columns\": {\"description\": \"An array of Columns that define the query\", \"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/Column\"}}, \"filters\": {\"description\": \"An optional array of Filters to apply to the query\", \"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/Filter\"}}}}, \"QueryOutput\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {}}}}, \"ReturnFormat\": {\"type\": \"string\", \"enum\": [\"OBJECTS\", \"ARRAYS\"]}, \"SetFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"values\", \"exclude\"], \"properties\": {\"values\": {\"type\": \"array\", \"items\": {}}, \"exclude\": {\"type\": \"boolean\"}}}]}, \"SortDirection\": {\"type\": \"string\", \"description\": \"The direction of the sort, either ascending or descending. If not supplied the default is ascending\", \"enum\": [\"ASC\", \"DESC\"]}, \"RelativeDateFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"units\"], \"properties\": {\"units\": {\"type\": \"string\", \"enum\": [\"MINUTES\", \"HOURS\", \"DAYS\", \"WEEKS\", \"MONTHS\", \"QUARTERS\", \"YEARS\"]}, \"pastCount\": {\"type\": \"integer\"}, \"futureCount\": {\"type\": \"integer\"}, \"anchor\": {\"allOf\": [{\"$ref\": \"#/components/schemas/DateObject\"}]}, \"includeNulls\": {\"type\": \"boolean\"}}}]}, \"TopNFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"direction, howMany, fieldToMeasure\"], \"properties\": {\"direction\": {\"enum\": [\"TOP\", \"BOTTOM\"], \"description\": \"Top (Ascending) or Bottom (Descending) N\"}, \"howMany\": {\"type\": \"integer\", \"description\": \"The number of values from the Top or the Bottom of the given fieldToMeasure\"}, \"fieldToMeasure\": {\"allOf\": [{\"$ref\": \"#/components/schemas/FilterColumn\"}]}}}]}}, \"few_shot_examples\": \"\\n\\\"sum of sales by segment\\\"\\n{\\n  \\\"columns\\\": [\\n    {\\n      \\\"columnName\\\": \\\"Segment\\\"\\n    },\\n    {\\n      \\\"columnName\\\": \\\"Sales\\\",\\n      \\\"function\\\": \\\"SUM\\\",\\n      \\\"maxDecimalPlaces\\\": 2\\n    }\\n  ]\\n}\\n\\\"What are the most profitable sub categories, and include the name of the category that each sub category belongs to.\\\"\\n\\n{\\n    \\\"columns\\\": [\\n      {\\n        \\\"columnName\\\": \\\"Category\\\"\\n      },\\n      {\\n        \\\"columnName\\\": \\\"Sub-Category\\\"\\n      },\\n      {\\n        \\\"columnName\\\": \\\"Profit Margin\\\",\\n        \\\"calculation\\\": \\\"SUM([Profit])/SUM([Sales])\\\",\\n        \\\"sortPriority\\\": 1,\\n        \\\"sortDirection\\\": \\\"DESC\\\"\\n      }\\n    ]\\n  }\\n\\\"what are the top selling Sub-Catetories? Remove individual sales below 200K\\n{\\n  \\\"columns\\\": [\\n    {\\n      \\\"columnName\\\": \\\"Sub-Category\\\"\\n    },\\n    {\\n      \\\"columnName\\\": \\\"Sales\\\",\\n      \\\"function\\\": \\\"SUM\\\",\\n      \\\"sortPriority\\\": 1,\\n      \\\"sortDirection\\\": \\\"DESC\\\"\\n    }\\n  ],\\n  \\\"filters\\\": [\\n      {\\n          \\\"columnName\\\": \\\"Sales\\\",\\n          \\\"filterType\\\": \\\"QUANTITATIVE\\\",\\n          \\\"quantitativeFilterType\\\": \\\"MIN\\\",\\n          \\\"min\\\": 200000\\n      }\\n  ]\\n}\", \"data_model\": [{\"columnName\": \"Calculation_1368249927221915648\", \"caption\": \"Profit Ratio\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"\"}, {\"columnName\": \"Category\", \"caption\": \"Category\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"City\", \"caption\": \"City\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Country/Region\", \"caption\": \"Country/Region\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Customer ID\", \"caption\": \"Customer ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Customer Name\", \"caption\": \"Customer Name\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Discount\", \"caption\": \"Discount\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Order Date\", \"caption\": \"Order Date\", \"dataType\": \"DATE\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Order ID (Returns)\", \"caption\": \"Order ID (Returns)\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Returns_7D45351C77614EC489F061F5420D292E\"}, {\"columnName\": \"Order ID\", \"caption\": \"Order ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Postal Code\", \"caption\": \"Postal Code\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Product ID\", \"caption\": \"Product ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Product Name\", \"caption\": \"Product Name\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Profit\", \"caption\": \"Profit\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Quantity\", \"caption\": \"Quantity\", \"dataType\": \"INTEGER\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Region (People)\", \"caption\": \"Region (People)\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"People_15C6BCE54C684AD3B15022F7106E99B9\"}, {\"columnName\": \"Region\", \"caption\": \"Region\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Regional Manager\", \"caption\": \"Regional Manager\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"People_15C6BCE54C684AD3B15022F7106E99B9\"}, {\"columnName\": \"Returned\", \"caption\": \"Returned\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Returns_7D45351C77614EC489F061F5420D292E\"}, {\"columnName\": \"Row ID\", \"caption\": \"Row ID\", \"dataType\": \"INTEGER\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Sales\", \"caption\": \"Sales\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Segment\", \"caption\": \"Segment\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Ship Date\", \"caption\": \"Ship Date\", \"dataType\": \"DATE\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Ship Mode\", \"caption\": \"Ship Mode\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"State/Province\", \"caption\": \"State/Province\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Sub-Category\", \"caption\": \"Sub-Category\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}]}\n"
     ]
    }
   ],
   "source": [
    "#load and instantiate prompt; link to base system prompt: https://drive.google.com/file/d/1wqmTSKIwDPgpucwLAgalEyumO2egDJJ5/view?usp=drive_link\n",
    "\n",
    "# Specify the path to the system prompt\n",
    "path = './prompts/nlq_to_vds.py'\n",
    "\n",
    "# read in system prompt\n",
    "with open(path, 'r') as json_file:\n",
    "    headless_bi_prompt_string = json_file.read()\n",
    "\n",
    "json_string = headless_bi_prompt_string.replace('prompt = ', '', 1).strip()\n",
    "\n",
    "headless_bi_prompt = json.loads(json_string)\n",
    "# add datasource metadata of the connected datasource to the system prompt\n",
    "headless_bi_prompt['data_model'] = datasource_metadata\n",
    "\n",
    "#convert prompt back to a string to pass in LLM request\n",
    "ready_prompt = json.dumps(headless_bi_prompt)\n",
    "print(ready_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3AivGN7iIe4_"
   },
   "outputs": [],
   "source": [
    "def get_utterance():\n",
    "  query = input('what would you like to know about your data? reply stop if you are done.')\n",
    "  return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv57JrlE6_zq",
    "outputId": "73a7549f-9657-4987-bdef-13b90e49a98a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. what are sales by region?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: To retrieve sales data by region, we need to query the `Region` and `Sales` columns. We'll use the SUM function to aggregate the sales data for each region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"sales_by_region = df.groupby('Region')['SUM(Sales)'].sum()\\nprint(sales_by_region)\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRegion\n",
      "Central    503170.67\n",
      "East       691828.17\n",
      "South      391721.91\n",
      "West       739813.61\n",
      "Name: SUM(Sales), dtype: float64\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe Python code grouped the dataframe `df` by the 'Region' column and calculated the sum of sales for each region. The result shows the total sales for each region:\n",
      "- Central: $503,170.67\n",
      "- East: $691,828.17\n",
      "- South: $391,721.91\n",
      "- West: $739,813.61\n",
      "\n",
      "This information provides the sales figures for each region in the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. what were the unprofitable orders in the corporate segment? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning:\n",
      "To identify unprofitable orders in the corporate segment, we need to retrieve order details where the profit is less than or equal to zero and the segment is \"Corporate\". The relevant columns would include \"Order ID\", \"Segment\", and \"Profit\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"unprofitable_orders = df[(df['Segment'] == 'Corporate') & (df['Profit'] < 0)]\\nunprofitable_orders\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m           Order ID    Segment    Profit\n",
      "0    US-2024-117723  Corporate -337.8060\n",
      "1    US-2024-103765  Corporate  -24.7716\n",
      "2    US-2022-117184  Corporate  -49.9200\n",
      "3    US-2024-113481  Corporate  -12.5280\n",
      "4    US-2023-152051  Corporate  -50.8704\n",
      "..              ...        ...       ...\n",
      "574  US-2024-158246  Corporate   -2.6997\n",
      "575  US-2024-169404  Corporate  -29.0536\n",
      "576  US-2023-144057  Corporate  -27.7920\n",
      "577  US-2022-125185  Corporate   -7.0980\n",
      "578  US-2022-131359  Corporate  -45.9954\n",
      "\n",
      "[562 rows x 3 columns]\u001b[0m\u001b[32;1m\u001b[1;3mThe unprofitable orders in the corporate segment are those where the profit is less than 0. There are a total of 562 unprofitable orders in the corporate segment. The table above shows a sample of these orders with columns for Order ID, Segment, and Profit.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. show me the top 10 most unprofitable orders in the corporate segment and which products were in the orders?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: To answer the query, we need to retrieve the top 10 most unprofitable orders in the corporate segment and the products included in those orders. We will include columns for the order ID, product name, segment, and profit. We'll filter by the corporate segment and sort the results to get the bottom 10 orders by profit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"unprofitable_orders = df[df['Segment'] == 'Corporate'].sort_values('Profit').head(10)\\nunprofitable_orders[['Order ID', 'Product Name', 'Profit']]\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m            Order ID                                       Product Name  \\\n",
      "804   US-2024-168116          Cubify CubeX 3D Printer Triple Head Print   \n",
      "2485  US-2024-122714               Ibico EPK-21 Electric Binding System   \n",
      "670   US-2024-148551   GBC Ibimaster 500 Manual ProClick Binding System   \n",
      "1489  US-2022-105571         GBC DocuBind TL300 Electric Binding System   \n",
      "2204  US-2024-159100                       Balt Solid Wood Round Tables   \n",
      "991   US-2022-146262                 Cisco 9971 IP Video Phone Charcoal   \n",
      "2460  US-2024-165344  Epson TM-T88V Direct Thermal Printer - Monochr...   \n",
      "1158  US-2021-108609                      Hon 94000 Series Round Tables   \n",
      "1977  US-2023-132479       GBC ProClick 150 Presentation Binding System   \n",
      "1142  US-2023-114013                Okidata MB491 Multifunction Printer   \n",
      "\n",
      "         Profit  \n",
      "804  -3839.9904  \n",
      "2485 -2929.4845  \n",
      "670  -1141.4700  \n",
      "1489 -1031.5385  \n",
      "2204  -968.8833  \n",
      "991   -950.4000  \n",
      "2460  -935.9595  \n",
      "1158  -734.5264  \n",
      "1977  -729.9138  \n",
      "1142  -643.7100  \u001b[0m\u001b[32;1m\u001b[1;3mI first filtered the dataframe to include only orders from the Corporate segment. Then, I sorted these orders by profit in ascending order and selected the top 10 most unprofitable orders. \n",
      "\n",
      "The top 10 most unprofitable orders in the Corporate segment are as follows:\n",
      "1. Order ID: US-2024-168116, Product Name: Cubify CubeX 3D Printer Triple Head Print, Profit: -3839.9904\n",
      "2. Order ID: US-2024-122714, Product Name: Ibico EPK-21 Electric Binding System, Profit: -2929.4845\n",
      "3. Order ID: US-2024-148551, Product Name: GBC Ibimaster 500 Manual ProClick Binding System, Profit: -1141.4700\n",
      "4. Order ID: US-2022-105571, Product Name: GBC DocuBind TL300 Electric Binding System, Profit: -1031.5385\n",
      "5. Order ID: US-2024-159100, Product Name: Balt Solid Wood Round Tables, Profit: -968.8833\n",
      "6. Order ID: US-2022-146262, Product Name: Cisco 9971 IP Video Phone Charcoal, Profit: -950.4000\n",
      "7. Order ID: US-2024-165344, Product Name: Epson TM-T88V Direct Thermal Printer - Monochrome, Profit: -935.9595\n",
      "8. Order ID: US-2021-108609, Product Name: Hon 94000 Series Round Tables, Profit: -734.5264\n",
      "9. Order ID: US-2023-132479, Product Name: GBC ProClick 150 Presentation Binding System, Profit: -729.9138\n",
      "10. Order ID: US-2023-114013, Product Name: Okidata MB491 Multifunction Printer, Profit: -643.7100\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. stop\n"
     ]
    }
   ],
   "source": [
    "active_utterance = get_utterance()\n",
    "while (active_utterance != 'stop'):\n",
    "  llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "  active_prompt_template = ChatPromptTemplate.from_messages([\n",
    "      SystemMessage(content=headless_bi_prompt_string),\n",
    "      (\"user\", \"{utterance}\")])\n",
    "  #get_utterance = input('what would you like to know about your data? ')\n",
    "  prompt_value = active_prompt_template.invoke(\n",
    "      {\n",
    "          \"utterance\": active_utterance\n",
    "      }\n",
    "  )\n",
    "  output_parser = StrOutputParser()\n",
    "  chain = active_prompt_template | llm | output_parser\n",
    "  output = chain.invoke(active_utterance)\n",
    "\n",
    "  #print reasoning\n",
    "  print(output.split('JSON_payload')[0])\n",
    "\n",
    "  #parse LLM output and query headless BI\n",
    "  parsed_output = output.split('JSON_payload')[1]\n",
    "  match = re.search(r'{.*}', parsed_output, re.DOTALL)\n",
    "  if match:\n",
    "    json_string = match.group(0)\n",
    "    payload = json.loads(json_string)\n",
    "  #print(payload)\n",
    "  df = send(payload)\n",
    "  agent = create_pandas_dataframe_agent(\n",
    "      ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\"),\n",
    "      df,\n",
    "      verbose=True,\n",
    "      agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "      allow_dangerous_code=True\n",
    "  )\n",
    "  instruct_header = \"answer the following query directly by executing the necessary python code. Give a succinct explanation of the steps you took and how you know the answer is correct: \"\n",
    "  agent.invoke(instruct_header + active_utterance)\n",
    "  active_utterance = get_utterance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "958jC30yhEBB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:langtab] *",
   "language": "python",
   "name": "conda-env-langtab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
