{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rj_FTCWyho2"
   },
   "source": [
    "# README\n",
    "This cookbook was created by Joe Constantino on June 25, 2024 to test an NLQ-to-VDS query pipeline for answering ad-hoc questions over Tableau Published Data Sources\n",
    "\n",
    "This cookbook can be configured to query a Tableau Published data source in natural language using the Headless BI REST endpoint. It uses langchain libraries and the langchain development framework to generate a headless BI request payload from a natural language input query. It then executes a chain to generate an answer to the user's input query.\n",
    "\n",
    "To get started with this notebook, make sure you've added your environment variables to your .env file. The environment variables are set up to consume a PAT token associated to a specific Tableau site. More info on authenticating to VizQL Data Service can be found here: https://help.tableau.com/v0.0/api/vizql-data-service/en-us/docs/vds_setup.html\n",
    "\n",
    "We recommend using gpt-4o or gpt-4o-mini for the VDS agent chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IJWw_bx_tpod"
   },
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# loading environment variables\n",
    "env_vars = os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KfyVOfaxzWrL"
   },
   "outputs": [],
   "source": [
    "#read in the datasource metadata of the datasource that is defined in your environment variables\n",
    "def read():\n",
    "    url = env_vars['READ_METADATA']\n",
    "    payload = json.dumps({\n",
    "        \"connection\": {\n",
    "            \"tableauServerName\": env_vars['TABLEAU_DOMAIN'],\n",
    "            \"siteId\": env_vars['SITE_NAME'],\n",
    "            \"datasource\": env_vars['DATA_SOURCE']\n",
    "        },\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Credential-Key': env_vars['PAT_NAME'],\n",
    "    'Credential-value': env_vars['PAT_SECRET'],\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Print the response data\n",
    "        #print(\"Response Data:\")\n",
    "        #print(response.json())\n",
    "        data = response.json()['data']\n",
    "        # Create a pandas DataFrame from the JSON data\n",
    "        #df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the first few rows of the DataFrame\n",
    "        #print(\"Table view of data from the public REST API:\")\n",
    "        #print(df.head())\n",
    "        #print(df.all())\n",
    "        return data\n",
    "        #display(df.head())\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
    "        print(response.text)\n",
    "datasource_metadata = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8vihYtKpuy_5"
   },
   "outputs": [],
   "source": [
    "#define the headless BI query template\n",
    "def send(query):\n",
    "    url = env_vars['VDS_URL']\n",
    "    payload = json.dumps({\n",
    "        \"connection\": {\n",
    "            \"tableauServerName\": env_vars['TABLEAU_DOMAIN'],\n",
    "            \"siteId\": env_vars['SITE_NAME'],\n",
    "            \"datasource\": env_vars['DATA_SOURCE']\n",
    "        },\n",
    "        \"query\": query\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Credential-Key': env_vars['PAT_NAME'],\n",
    "    'Credential-value': env_vars['PAT_SECRET'],\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Print the response data\n",
    "        #print(\"Response Data:\")\n",
    "        #print(response.json())\n",
    "        data = response.json()['data']\n",
    "        # Create a pandas DataFrame from the JSON data\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the first few rows of the DataFrame\n",
    "        #print(\"Table view of data from the public REST API:\")\n",
    "        #print(df.head())\n",
    "        #print(df.all())\n",
    "        return df\n",
    "        #display(df.head())\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the API. Status code:\", response.status_code)\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YoQOU_fmvelA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instructions\": \"You are an expert at writing JSON payloads for VDS queries. \\nVDS queries are passed in as a JSON body that describes the query, and the return is passed back as an array of JSON objects. The VDS query is a JSON object. It contains two fundamental components.\\n1. columns [required] - an array of columns that define the desired output of the query\\n2. filters [optional] - an array of filters to apply to the query. They can include fields that are not in the columns array.\\nYour task is to retrieve data relevant to a natural language query. A pandas AI Agent will be used to transform and analyze the data that your VDS query returns. Don't try to do too much with the json query. You will be successful if you bring back all the data that is required to answer the question, even if additional transformation and actions are needed. Query as much data as might be useful; it's ok if you pull in superfluous columns, but only pull in columns based on what is available in the data model. You will construct a VDS query from a natural language query.\\nYou will ground the natural language query in the data model provided in the prompt.You have to map the intent of the query to the exact string that are listed in the 'data model' section.\\nUse the natural language query and the provided datamodel to generate the appropriate JSON object for a VDS query.\\nKeep your ouput very structured. Use the following structure:\\nReasoning: \\nJSON_payload:\\nMake sure you use this structure so that it's simple to parse the output.\\n\", \"user_query\": \"\", \"vds_schema\": {\"Column\": {\"type\": \"object\", \"anyOf\": [{\"required\": [\"columnName\"]}, {\"required\": [\"columnName\", \"function\"]}, {\"required\": [\"columnName\", \"calculation\"]}], \"properties\": {\"columnName\": {\"type\": \"string\", \"description\": \"The name of the column which must be supplied. Either a reference to a specific column in the data source, or in the case of a calculation a user supplied name for the calculation.\"}, \"columnAlias\": {\"type\": \"string\", \"description\": \"An alternate name to give the column. Will only be used in Object format output.\"}, \"maxDecimalPlaces\": {\"type\": \"integer\", \"description\": \"The maximum number of decimal places. Any trailing 0s will be dropped. The maxDecimalPlaces value must be greater or equal to 0.\"}, \"sortDirection\": {\"allOf\": [{\"$ref\": \"#/components/schemas/SortDirection\"}, {\"description\": \"The direction of the sort, either ascending or descending. If not supplied the default is ascending\"}]}, \"sortPriority\": {\"type\": \"integer\", \"description\": \"To enable sorting on a specific Column provide a sortPriority for that Column, and that Column will be sorted. The sortPriority provides a ranking of how to sort Columns when multiple Columns are being sorted. The highest priority (lowest number) Column is sorted first. If only 1 Column is being sorted, then any value may be used for sortPriority. SortPriority should be an integer greater than 0.\"}, \"function\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Function\"}, {\"description\": \"Provide a Function for a Column to generate an aggregation against that Columns' values. For example providing the SUM Function will cause an aggregated SUM to be calculated for that Column.\"}]}, \"calculation\": {\"type\": \"string\", \"description\": \"Provide a Calculation to generate a new data Column based on that Calculation. The Calcuation should contain a string based on the Tableau Calculated Field Syntax. Since this is a newly generated Column, you must give it its own unique Column Name. Also a Column cannot contain both a Function, and a Calculation.\"}}}, \"ColumnMetadata\": {\"type\": \"object\", \"description\": \"Describes a column in the datasource that can be used to create queries.\", \"properties\": {\"columnName\": {\"type\": \"string\"}, \"caption\": {\"type\": \"string\"}, \"dataType\": {\"type\": \"string\", \"enum\": [\"UNSPECIFIED\", \"INTEGER\", \"REAL\", \"STRING\", \"DATETIME\", \"BOOLEAN\", \"DATE\", \"SPATIAL\", \"UNKNOWN\", \"UNRECOGNIZED\"]}, \"columnContainsNulls\": {\"type\": \"boolean\"}, \"objectGraphId\": {\"type\": \"string\"}}}, \"DateObject\": {\"type\": \"object\", \"required\": [\"day\", \"month\", \"year\"], \"properties\": {\"day\": {\"type\": \"integer\"}, \"month\": {\"type\": \"integer\"}, \"year\": {\"type\": \"integer\"}}}, \"Filter\": {\"type\": \"object\", \"required\": [\"filterType\"], \"properties\": {\"columnName\": {\"type\": \"string\"}, \"column\": {\"allOf\": [{\"$ref\": \"#/components/schemas/FilterColumn\"}]}, \"filterType\": {\"type\": \"string\", \"enum\": [\"QUANTITATIVE\", \"SET\", \"DATE\", \"TOP\"]}, \"context\": {\"type\": \"boolean\", \"default\": \"false\"}}, \"discriminator\": {\"propertyName\": \"filterType\", \"mapping\": {\"QUANTITATIVE\": \"#/components/schemas/QuantitativeFilter\", \"SET\": \"#/components/schemas/SetFilter\", \"DATE\": \"#/components/schemas/RelativeDateFilter\", \"TOP\": \"#/components/schemas/TopNFilter\"}}}, \"FilterColumn\": {\"type\": \"object\", \"oneOf\": [{\"required\": [\"columnName\", \"function\"]}, {\"required\": [\"calculation\"]}], \"properties\": {\"columnName\": {\"type\": \"string\"}, \"function\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Function\"}]}, \"calculation\": {\"type\": \"string\"}}}, \"Function\": {\"type\": \"string\", \"description\": \"The standard set of Tableau aggregations.\", \"enum\": [\"SUM\", \"AVG\", \"MEDIAN\", \"COUNT\", \"COUNT_DIST\", \"MIN\", \"MAX\", \"STD_DEV\", \"VARIANCE\", \"CLCT\", \"DATE_YEAR\", \"DATE_QTR\", \"DATE_MONTH\", \"DATE_WEEK\", \"DATE_DAY\", \"DATE_TRUNC_YEAR\", \"DATE_TRUNC_QTR\", \"DATE_TRUNC_MONTH\", \"DATE_TRUNC_WEEK\", \"DATE_TRUNC_DAY\"]}, \"MetadataOutput\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/ColumnMetadata\"}}}}, \"QuantitativeFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"quantitativeFilterType\"], \"properties\": {\"quantitativeFilterType\": {\"type\": \"string\", \"enum\": [\"RANGE\", \"MIN\", \"MAX\", \"SPECIAL\"]}, \"min\": {\"type\": \"number\", \"description\": \"A numerical value, either integer or floating point indicating the minimum value to filter upon. Required for RANGE and MIN\"}, \"max\": {\"type\": \"number\", \"description\": \"A numerical value, either integer or floating point indicating the maximum value to filter upon. Required for RANGE and MAX\"}, \"minDate\": {\"$ref\": \"#/components/schemas/DateObject\"}, \"maxDate\": {\"$ref\": \"#/components/schemas/DateObject\"}, \"quantitativeFilterIncludedValues\": {\"allOf\": [{\"$ref\": \"#/components/schemas/QuantitativeFilterIncludedValues\"}]}}}]}, \"QuantitativeFilterIncludedValues\": {\"type\": \"string\", \"enum\": [\"ALL\", \"NON_NULL\", \"NULL\", \"IN_RANGE\", \"IN_RANGE_OR_NULL\", \"NONE\"]}, \"Query\": {\"description\": \"The Query is the fundamental interface to VDS. It holds the specific semantics to perform against the Data Source. A Query consists of an array of Columns to query against, an optional array of filters to apply to the query, and an optional Metadata field to modify the query behavior.\", \"required\": [\"columns\"], \"type\": \"object\", \"properties\": {\"columns\": {\"description\": \"An array of Columns that define the query\", \"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/Column\"}}, \"filters\": {\"description\": \"An optional array of Filters to apply to the query\", \"type\": \"array\", \"items\": {\"$ref\": \"#/components/schemas/Filter\"}}}}, \"QueryOutput\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {}}}}, \"ReturnFormat\": {\"type\": \"string\", \"enum\": [\"OBJECTS\", \"ARRAYS\"]}, \"SetFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"values\", \"exclude\"], \"properties\": {\"values\": {\"type\": \"array\", \"items\": {}}, \"exclude\": {\"type\": \"boolean\"}}}]}, \"SortDirection\": {\"type\": \"string\", \"description\": \"The direction of the sort, either ascending or descending. If not supplied the default is ascending\", \"enum\": [\"ASC\", \"DESC\"]}, \"RelativeDateFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"units\"], \"properties\": {\"units\": {\"type\": \"string\", \"enum\": [\"MINUTES\", \"HOURS\", \"DAYS\", \"WEEKS\", \"MONTHS\", \"QUARTERS\", \"YEARS\"]}, \"pastCount\": {\"type\": \"integer\"}, \"futureCount\": {\"type\": \"integer\"}, \"anchor\": {\"allOf\": [{\"$ref\": \"#/components/schemas/DateObject\"}]}, \"includeNulls\": {\"type\": \"boolean\"}}}]}, \"TopNFilter\": {\"allOf\": [{\"$ref\": \"#/components/schemas/Filter\"}, {\"type\": \"object\", \"required\": [\"direction, howMany, fieldToMeasure\"], \"properties\": {\"direction\": {\"enum\": [\"TOP\", \"BOTTOM\"], \"description\": \"Top (Ascending) or Bottom (Descending) N\"}, \"howMany\": {\"type\": \"integer\", \"description\": \"The number of values from the Top or the Bottom of the given fieldToMeasure\"}, \"fieldToMeasure\": {\"allOf\": [{\"$ref\": \"#/components/schemas/FilterColumn\"}]}}}]}}, \"few_shot_examples\": \"\\n\\\"sum of sales by segment\\\"\\n{\\n  \\\"columns\\\": [\\n    {\\n      \\\"columnName\\\": \\\"Segment\\\"\\n    },\\n    {\\n      \\\"columnName\\\": \\\"Sales\\\",\\n      \\\"function\\\": \\\"SUM\\\",\\n      \\\"maxDecimalPlaces\\\": 2\\n    }\\n  ]\\n}\\n\\\"What are the most profitable sub categories, and include the name of the category that each sub category belongs to.\\\"\\n\\n{\\n    \\\"columns\\\": [\\n      {\\n        \\\"columnName\\\": \\\"Category\\\"\\n      },\\n      {\\n        \\\"columnName\\\": \\\"Sub-Category\\\"\\n      },\\n      {\\n        \\\"columnName\\\": \\\"Profit Margin\\\",\\n        \\\"calculation\\\": \\\"SUM([Profit])/SUM([Sales])\\\",\\n        \\\"sortPriority\\\": 1,\\n        \\\"sortDirection\\\": \\\"DESC\\\"\\n      }\\n    ]\\n  }\\n\\\"what are the top selling Sub-Catetories? Remove individual sales below 200K\\n{\\n  \\\"columns\\\": [\\n    {\\n      \\\"columnName\\\": \\\"Sub-Category\\\"\\n    },\\n    {\\n      \\\"columnName\\\": \\\"Sales\\\",\\n      \\\"function\\\": \\\"SUM\\\",\\n      \\\"sortPriority\\\": 1,\\n      \\\"sortDirection\\\": \\\"DESC\\\"\\n    }\\n  ],\\n  \\\"filters\\\": [\\n      {\\n          \\\"columnName\\\": \\\"Sales\\\",\\n          \\\"filterType\\\": \\\"QUANTITATIVE\\\",\\n          \\\"quantitativeFilterType\\\": \\\"MIN\\\",\\n          \\\"min\\\": 200000\\n      }\\n  ]\\n}\", \"data_model\": [{\"columnName\": \"Calculation_1368249927221915648\", \"caption\": \"Profit Ratio\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"\"}, {\"columnName\": \"Category\", \"caption\": \"Category\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"City\", \"caption\": \"City\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Country/Region\", \"caption\": \"Country/Region\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Customer ID\", \"caption\": \"Customer ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Customer Name\", \"caption\": \"Customer Name\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Discount\", \"caption\": \"Discount\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Order Date\", \"caption\": \"Order Date\", \"dataType\": \"DATE\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Order ID (Returns)\", \"caption\": \"Order ID (Returns)\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Returns_7D45351C77614EC489F061F5420D292E\"}, {\"columnName\": \"Order ID\", \"caption\": \"Order ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Postal Code\", \"caption\": \"Postal Code\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Product ID\", \"caption\": \"Product ID\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Product Name\", \"caption\": \"Product Name\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Profit\", \"caption\": \"Profit\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Quantity\", \"caption\": \"Quantity\", \"dataType\": \"INTEGER\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Region (People)\", \"caption\": \"Region (People)\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"People_15C6BCE54C684AD3B15022F7106E99B9\"}, {\"columnName\": \"Region\", \"caption\": \"Region\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Regional Manager\", \"caption\": \"Regional Manager\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"People_15C6BCE54C684AD3B15022F7106E99B9\"}, {\"columnName\": \"Returned\", \"caption\": \"Returned\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Returns_7D45351C77614EC489F061F5420D292E\"}, {\"columnName\": \"Row ID\", \"caption\": \"Row ID\", \"dataType\": \"INTEGER\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Sales\", \"caption\": \"Sales\", \"dataType\": \"REAL\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Segment\", \"caption\": \"Segment\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Ship Date\", \"caption\": \"Ship Date\", \"dataType\": \"DATE\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Ship Mode\", \"caption\": \"Ship Mode\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"State/Province\", \"caption\": \"State/Province\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}, {\"columnName\": \"Sub-Category\", \"caption\": \"Sub-Category\", \"dataType\": \"STRING\", \"columnContainsNulls\": true, \"objectGraphId\": \"Orders_D40E6F69FB37401BB041729A558005D5\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the system prompt\n",
    "path = './prompts/nlq_to_vds.py'\n",
    "\n",
    "# read in system prompt\n",
    "with open(path, 'r') as json_file:\n",
    "    headless_bi_prompt_string = json_file.read()\n",
    "\n",
    "#remove \"prompt = \" prefix from the string\n",
    "stripped_headless_bi_prompt_string = headless_bi_prompt_string.replace('prompt = ', '', 1).strip()\n",
    "\n",
    "#convert headless bi prompt string to a json object\n",
    "headless_bi_prompt = json.loads(stripped_headless_bi_prompt_string)\n",
    "\n",
    "# add the datasource metadata of the connected datasource to the system prompt\n",
    "headless_bi_prompt['data_model'] = datasource_metadata\n",
    "\n",
    "#convert prompt back to a string to pass in LLM request\n",
    "ready_prompt = json.dumps(headless_bi_prompt)\n",
    "print(ready_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3AivGN7iIe4_"
   },
   "outputs": [],
   "source": [
    "#define function for getting user queries\n",
    "def get_utterance():\n",
    "  query = input('what would you like to know about your data? reply stop if you are done.')\n",
    "  return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv57JrlE6_zq",
    "outputId": "73a7549f-9657-4987-bdef-13b90e49a98a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. what are sales per region? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The user is asking for sales data categorized by region. Therefore, we need to include the \"Region\" column and aggregate the \"Sales\" column using the SUM function to get the total sales per region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"sales_per_region = df.set_index('Region')['SUM(Sales)']\\nsales_per_region\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRegion\n",
      "West       739813.61\n",
      "East       691828.17\n",
      "Central    503170.67\n",
      "South      391721.91\n",
      "Name: SUM(Sales), dtype: float64\u001b[0m\u001b[32;1m\u001b[1;3mTo find the sales per region, I executed the following steps:\n",
      "\n",
      "1. **Set the Index**: I set the 'Region' column as the index of the dataframe using `set_index('Region')`. This allows us to easily reference sales data by region.\n",
      "2. **Select Sales Data**: I accessed the 'SUM(Sales)' column to get the sales figures corresponding to each region.\n",
      "3. **Result**: The result is a Series with regions as the index and their corresponding sales values.\n",
      "\n",
      "The output shows the sales figures for each region, confirming that the calculations are based on the original dataframe. The values are consistent with the data provided in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what would you like to know about your data? reply stop if you are done. which products are the most discounted? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: To identify the products that are most discounted, we will need to retrieve data on product names and their respective discount amounts. The query will include the \"Product\" column and the \"Discount\" column, applying a sorting function to display products in descending order based on their discount amount.\n",
      "\n",
      "\n",
      "Failed to fetch data from the API. Status code: 400\n",
      "{\"errorCode\":\"400002\",\"message\":\"Column 'Product' does not exist\",\"datetime\":\"2024-08-25T14:24:55.573619Z\",\"details\":{\"errorCode\":\"500104\",\"message\":\"Base column [DSInternalName].[Product] does not exist.\",\"datetime\":\"2024-08-25T14:24:55.489511Z\",\"details\":null}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected pandas DataFrame, got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#print(payload)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m send(payload)\n\u001b[0;32m---> 28\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pandas_dataframe_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPANDAS_AGENT_MODEL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAgentType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPENAI_FUNCTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dangerous_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m instruct_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer the following query directly by executing the necessary python code. Give a succinct explanation of the steps you took and how you know the answer is correct: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m agent\u001b[38;5;241m.\u001b[39minvoke(instruct_header \u001b[38;5;241m+\u001b[39m active_utterance)\n",
      "File \u001b[0;32m/usr/local/Caskroom/mambaforge/base/envs/langtab/lib/python3.12/site-packages/langchain_experimental/agents/agent_toolkits/pandas/base.py:277\u001b[0m, in \u001b[0;36mcreate_pandas_dataframe_agent\u001b[0;34m(llm, df, agent_type, callback_manager, prefix, suffix, input_variables, verbose, return_intermediate_steps, max_iterations, max_execution_time, early_stopping_method, agent_executor_kwargs, include_df_in_prompt, number_of_head_rows, extra_tools, engine, allow_dangerous_code, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _df \u001b[38;5;129;01min\u001b[39;00m df \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [df]:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_df, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected pandas DataFrame, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_variables:\n\u001b[1;32m    280\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Expected pandas DataFrame, got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "active_utterance = get_utterance()\n",
    "\n",
    "while (active_utterance != 'stop'):\n",
    "  llm = ChatOpenAI(model=env_vars['VDS_AGENT_MODEL'])\n",
    "  active_prompt_template = ChatPromptTemplate.from_messages([\n",
    "      SystemMessage(content=headless_bi_prompt_string),\n",
    "      (\"user\", \"{utterance}\")])\n",
    "  prompt_value = active_prompt_template.invoke(\n",
    "      {\n",
    "          \"utterance\": active_utterance\n",
    "      }\n",
    "  )\n",
    "  output_parser = StrOutputParser()\n",
    "  chain = active_prompt_template | llm | output_parser\n",
    "  output = chain.invoke(active_utterance)\n",
    "\n",
    "  #print reasoning\n",
    "  print(output.split('JSON_payload')[0])\n",
    "\n",
    "  #parse LLM output and query headless BI\n",
    "  parsed_output = output.split('JSON_payload')[1]\n",
    "  match = re.search(r'{.*}', parsed_output, re.DOTALL)\n",
    "  if match:\n",
    "    json_string = match.group(0)\n",
    "    payload = json.loads(json_string)\n",
    "  #print(payload)\n",
    "  df = send(payload)\n",
    "  agent = create_pandas_dataframe_agent(\n",
    "      ChatOpenAI(temperature=0, model=env_vars['PANDAS_AGENT_MODEL']),\n",
    "      df,\n",
    "      verbose=True,\n",
    "      agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "      allow_dangerous_code=True\n",
    "  )\n",
    "  instruct_header = \"answer the following query directly by executing the necessary python code. Give a succinct explanation of the steps you took and how you know the answer is correct: \"\n",
    "  agent.invoke(instruct_header + active_utterance)\n",
    "  active_utterance = get_utterance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "958jC30yhEBB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: To determine which products tend to have the largest discounts, I will query the relevant columns that represent the products and their discount values. I will include the product name and the discount amount, and I will also sort the results based on the discount amount in descending order to identify the products with the largest discounts.\n",
      "\n",
      "\n",
      "{'columns': [{'columnName': 'Product'}, {'columnName': 'Discount Amount', 'function': 'MAX'}], 'filters': []}\n"
     ]
    }
   ],
   "source": [
    "#test nlq_to_vds chain\n",
    "test_utterance = \"which products tend to have the largest discounts?\"\n",
    "llm = ChatOpenAI(model=env_vars['VDS_AGENT_MODEL'])\n",
    "active_prompt_template = ChatPromptTemplate.from_messages([\n",
    "  SystemMessage(content=headless_bi_prompt_string),\n",
    "  (\"user\", \"{utterance}\")])\n",
    "prompt_value = active_prompt_template.invoke(\n",
    "  {\n",
    "      \"utterance\": test_utterance\n",
    "  }\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "chain = active_prompt_template | llm | output_parser\n",
    "output = chain.invoke(test_utterance)\n",
    "\n",
    "#print reasoning\n",
    "print(output.split('JSON_payload')[0])\n",
    "\n",
    "#parse LLM output and print generated json\n",
    "parsed_output = output.split('JSON_payload')[1]\n",
    "match = re.search(r'{.*}', parsed_output, re.DOTALL)\n",
    "if match:\n",
    "    json_string = match.group(0)\n",
    "json_payload = json.loads(json_string)\n",
    "print(json_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:langtab] *",
   "language": "python",
   "name": "conda-env-langtab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
